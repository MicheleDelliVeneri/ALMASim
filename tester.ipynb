{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'simulation' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n simulation ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import simulator as sm\n",
    "import illustris_python as il\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import subprocess\n",
    "file = 'morphologies_deeplearn.hdf5'\n",
    "db = sm.get_data_from_hdf(file)\n",
    "subhalo_ids = db['SubhaloID'].values\n",
    "print('Database contains %d subhalos' % len(subhalo_ids))\n",
    "basePath = '/home/astro/Documents/TNGData'\n",
    "snapNum = 99\n",
    "\n",
    "import six\n",
    "from os.path import isfile,expanduser\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def partTypeNum(partType):\n",
    "    \"\"\" Mapping between common names and numeric particle types. \"\"\"\n",
    "    if str(partType).isdigit():\n",
    "        return int(partType)\n",
    "        \n",
    "    if str(partType).lower() in ['gas','cells']:\n",
    "        return 0\n",
    "    if str(partType).lower() in ['dm','darkmatter']:\n",
    "        return 1\n",
    "    if str(partType).lower() in ['dmlowres']:\n",
    "        return 2 # only zoom simulations, not present in full periodic boxes\n",
    "    if str(partType).lower() in ['tracer','tracers','tracermc','trmc']:\n",
    "        return 3\n",
    "    if str(partType).lower() in ['star','stars','stellar']:\n",
    "        return 4 # only those with GFM_StellarFormationTime>0\n",
    "    if str(partType).lower() in ['wind']:\n",
    "        return 4 # only those with GFM_StellarFormationTime<0\n",
    "    if str(partType).lower() in ['bh','bhs','blackhole','blackholes']:\n",
    "        return 5\n",
    "    \n",
    "    raise Exception(\"Unknown particle type name.\")\n",
    "\n",
    "\n",
    "def gcPath(basePath, snapNum, chunkNum=0):\n",
    "    \"\"\" Return absolute path to a group catalog HDF5 file (modify as needed). \"\"\"\n",
    "    gcPath = basePath + '/groups_%03d/' % snapNum\n",
    "    filePath1 = gcPath + 'groups_%03d.%d.hdf5' % (snapNum, chunkNum)\n",
    "    filePath2 = gcPath + 'fof_subhalo_tab_%03d.%d.hdf5' % (snapNum, chunkNum)\n",
    "\n",
    "    if isfile(expanduser(filePath1)):\n",
    "        return filePath1\n",
    "    return filePath2\n",
    "\n",
    "\n",
    "def offsetPath(basePath, snapNum):\n",
    "    \"\"\" Return absolute path to a separate offset file (modify as needed). \"\"\"\n",
    "    offsetPath = basePath + '/../postprocessing/offsets/offsets_%03d.hdf5' % snapNum\n",
    "\n",
    "    return offsetPath\n",
    "\n",
    "\n",
    "def loadObjects(basePath, snapNum, gName, nName, fields):\n",
    "    \"\"\" Load either halo or subhalo information from the group catalog. \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # make sure fields is not a single element\n",
    "    if isinstance(fields, six.string_types):\n",
    "        fields = [fields]\n",
    "\n",
    "    # load header from first chunk\n",
    "    with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "\n",
    "        if 'N'+nName+'_Total' not in header and nName == 'subgroups':\n",
    "            nName = 'subhalos' # alternate convention\n",
    "\n",
    "        result['count'] = f['Header'].attrs['N' + nName + '_Total']\n",
    "\n",
    "        if not result['count']:\n",
    "            print('warning: zero groups, empty return (snap=' + str(snapNum) + ').')\n",
    "            return result\n",
    "\n",
    "        # if fields not specified, load everything\n",
    "        if not fields:\n",
    "            fields = list(f[gName].keys())\n",
    "\n",
    "        for field in fields:\n",
    "            # verify existence\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Group catalog does not have requested field [\" + field + \"]!\")\n",
    "\n",
    "            # replace local length with global\n",
    "            shape = list(f[gName][field].shape)\n",
    "            shape[0] = result['count']\n",
    "\n",
    "            # allocate within return dict\n",
    "            result[field] = np.zeros(shape, dtype=f[gName][field].dtype)\n",
    "\n",
    "    # loop over chunks\n",
    "    wOffset = 0\n",
    "\n",
    "    for i in range(header['NumFiles']):\n",
    "        f = h5py.File(gcPath(basePath, snapNum, i), 'r')\n",
    "\n",
    "        if not f['Header'].attrs['N'+nName+'_ThisFile']:\n",
    "            continue  # empty file chunk\n",
    "\n",
    "        # loop over each requested field\n",
    "        for field in fields:\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Group catalog does not have requested field [\" + field + \"]!\")\n",
    "\n",
    "            # shape and type\n",
    "            shape = f[gName][field].shape\n",
    "\n",
    "            # read data local to the current file\n",
    "            if len(shape) == 1:\n",
    "                result[field][wOffset:wOffset+shape[0]] = f[gName][field][0:shape[0]]\n",
    "            else:\n",
    "                result[field][wOffset:wOffset+shape[0], :] = f[gName][field][0:shape[0], :]\n",
    "\n",
    "        wOffset += shape[0]\n",
    "        f.close()\n",
    "\n",
    "    # only a single field? then return the array instead of a single item dict\n",
    "    if len(fields) == 1:\n",
    "        return result[fields[0]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def loadSubhalos(basePath, snapNum, fields=None):\n",
    "    \"\"\" Load all subhalo information from the entire group catalog for one snapshot\n",
    "       (optionally restrict to a subset given by fields). \"\"\"\n",
    "\n",
    "    return loadObjects(basePath, snapNum, \"Subhalo\", \"subgroups\", fields)\n",
    "\n",
    "\n",
    "def loadHalos(basePath, snapNum, fields=None):\n",
    "    \"\"\" Load all halo information from the entire group catalog for one snapshot\n",
    "       (optionally restrict to a subset given by fields). \"\"\"\n",
    "\n",
    "    return loadObjects(basePath, snapNum, \"Group\", \"groups\", fields)\n",
    "\n",
    "\n",
    "def loadHeader(basePath, snapNum):\n",
    "    \"\"\" Load the group catalog header. \"\"\"\n",
    "    with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "\n",
    "    return header\n",
    "\n",
    "\n",
    "def load(basePath, snapNum):\n",
    "    \"\"\" Load complete group catalog all at once. \"\"\"\n",
    "    r = {}\n",
    "    r['subhalos'] = loadSubhalos(basePath, snapNum)\n",
    "    r['halos']    = loadHalos(basePath, snapNum)\n",
    "    r['header']   = loadHeader(basePath, snapNum)\n",
    "    return r\n",
    "\n",
    "\n",
    "def loadSingle(basePath, snapNum, haloID=-1, subhaloID=-1):\n",
    "    \"\"\" Return complete group catalog information for one halo or subhalo. \"\"\"\n",
    "    if (haloID < 0 and subhaloID < 0) or (haloID >= 0 and subhaloID >= 0):\n",
    "        raise Exception(\"Must specify either haloID or subhaloID (and not both).\")\n",
    "\n",
    "    gName = \"Subhalo\" if subhaloID >= 0 else \"Group\"\n",
    "    searchID = subhaloID if subhaloID >= 0 else haloID\n",
    "\n",
    "    # old or new format\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        # use separate 'offsets_nnn.hdf5' files\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            offsets = f['FileOffsets/'+gName][()]\n",
    "    else:\n",
    "        # use header of group catalog\n",
    "        with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "            offsets = f['Header'].attrs['FileOffsets_'+gName]\n",
    "\n",
    "    offsets = searchID - offsets\n",
    "    fileNum = np.max(np.where(offsets >= 0))\n",
    "    groupOffset = offsets[fileNum]\n",
    "\n",
    "    # load halo/subhalo fields into a dict\n",
    "    result = {}\n",
    "\n",
    "    with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "        for haloProp in f[gName].keys():\n",
    "            result[haloProp] = f[gName][haloProp][groupOffset]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def snapPath(basePath, snapNum, chunkNum=0):\n",
    "    \"\"\" Return absolute path to a snapshot HDF5 file (modify as needed). \"\"\"\n",
    "    snapPath = basePath + '/snapdir_' + str(snapNum).zfill(3) + '/'\n",
    "    filePath1 = snapPath + 'snap_' + str(snapNum).zfill(3) + '.' + str(chunkNum) + '.hdf5'\n",
    "    filePath2 = filePath1.replace('/snap_', '/snapshot_')\n",
    "\n",
    "    if isfile(filePath1):\n",
    "        return filePath1\n",
    "    return filePath2\n",
    "\n",
    "\n",
    "def getNumPart(header):\n",
    "    \"\"\" Calculate number of particles of all types given a snapshot header. \"\"\"\n",
    "    if 'NumPart_Total_HighWord' not in header:\n",
    "        return header['NumPart_Total'] # new uint64 convention\n",
    "\n",
    "    nTypes = 6\n",
    "\n",
    "    nPart = np.zeros(nTypes, dtype=np.int64)\n",
    "    for j in range(nTypes):\n",
    "        nPart[j] = header['NumPart_Total'][j] | (header['NumPart_Total_HighWord'][j] << 32)\n",
    "\n",
    "    return nPart\n",
    "\n",
    "\n",
    "def loadSubset(basePath, snapNum, partType, fields=None, subset=None, mdi=None, sq=True, float32=False):\n",
    "    \"\"\" Load a subset of fields for all particles/cells of a given partType.\n",
    "        If offset and length specified, load only that subset of the partType.\n",
    "        If mdi is specified, must be a list of integers of the same length as fields,\n",
    "        giving for each field the multi-dimensional index (on the second dimension) to load.\n",
    "          For example, fields=['Coordinates', 'Masses'] and mdi=[1, None] returns a 1D array\n",
    "          of y-Coordinates only, together with Masses.\n",
    "        If sq is True, return a numpy array instead of a dict if len(fields)==1.\n",
    "        If float32 is True, load any float64 datatype arrays directly as float32 (save memory). \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    ptNum = partTypeNum(partType)\n",
    "    gName = \"PartType\" + str(ptNum)\n",
    "\n",
    "    # make sure fields is not a single element\n",
    "    if isinstance(fields, six.string_types):\n",
    "        fields = [fields]\n",
    "\n",
    "    # load header from first chunk\n",
    "    with h5py.File(snapPath(basePath, snapNum), 'r') as f:\n",
    "\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "        nPart = getNumPart(header)\n",
    "\n",
    "        # decide global read size, starting file chunk, and starting file chunk offset\n",
    "        if subset:\n",
    "            offsetsThisType = subset['offsetType'][ptNum] - subset['snapOffsets'][ptNum, :]\n",
    "\n",
    "            fileNum = np.max(np.where(offsetsThisType >= 0))\n",
    "            fileOff = offsetsThisType[fileNum]\n",
    "            numToRead = subset['lenType'][ptNum]\n",
    "        else:\n",
    "            fileNum = 0\n",
    "            fileOff = 0\n",
    "            numToRead = nPart[ptNum]\n",
    "\n",
    "        result['count'] = numToRead\n",
    "\n",
    "        if not numToRead:\n",
    "            # print('warning: no particles of requested type, empty return.')\n",
    "            return result\n",
    "\n",
    "        # find a chunk with this particle type\n",
    "        i = 1\n",
    "        while gName not in f:\n",
    "            if os.path.isfile(snapPath(basePath, snapNum, i)):\n",
    "                f = h5py.File(snapPath(basePath, snapNum, i), 'r')\n",
    "            else:\n",
    "                api_key = '8f578b92e700fae3266931f4d785f82c'\n",
    "                url = f'http://www.tng-project.org/api/TNG100-1/files/snapshot-{str(snapNum)}'\n",
    "                subdir = os.path.join('output', 'snapdir_0{}'.format(str(i)))\n",
    "                cmd = f'wget -q --progress=bar  --content-disposition --header=\"API-Key:{api_key}\" {url}.{i}.hdf5'\n",
    "                print(f'Downloading {message} {i} ...')\n",
    "                subprocess.check_call(cmd, shell=True)\n",
    "                print('Done.')\n",
    "                f = h5py.File(snapPath(basePath, snapNum, i), 'r')\n",
    "            i += 1\n",
    "\n",
    "        # if fields not specified, load everything\n",
    "        if not fields:\n",
    "            fields = list(f[gName].keys())\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            # verify existence\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Particle type [\"+str(ptNum)+\"] does not have field [\"+field+\"]\")\n",
    "\n",
    "            # replace local length with global\n",
    "            shape = list(f[gName][field].shape)\n",
    "            shape[0] = numToRead\n",
    "\n",
    "            # multi-dimensional index slice load\n",
    "            if mdi is not None and mdi[i] is not None:\n",
    "                if len(shape) != 2:\n",
    "                    raise Exception(\"Read error: mdi requested on non-2D field [\"+field+\"]\")\n",
    "                shape = [shape[0]]\n",
    "\n",
    "            # allocate within return dict\n",
    "            dtype = f[gName][field].dtype\n",
    "            if dtype == np.float64 and float32: dtype = np.float32\n",
    "            result[field] = np.zeros(shape, dtype=dtype)\n",
    "\n",
    "    # loop over chunks\n",
    "    wOffset = 0\n",
    "    origNumToRead = numToRead\n",
    "\n",
    "    while numToRead:\n",
    "        if not os.path.isfile(snapPath(basePath, snapNum, fileNum)):\n",
    "            # move directory to the correct directory data !!!\n",
    "            api_key = '8f578b92e700fae3266931f4d785f82c'\n",
    "            url = f'http://www.tng-project.org/api/TNG100-1/files/snapshot-{str(snapNum)}'\n",
    "            subdir = os.path.join('output', 'snapdir_0{}'.format(str(fileNum)))\n",
    "            cmd = f'wget -q --progress=bar  --content-disposition --header=\"API-Key:{api_key}\" {url}.{fileNum}.hdf5'\n",
    "            print(f'Downloading Snapshot {fileNum} ...')\n",
    "            subprocess.check_call(cmd, shell=True)\n",
    "            print('Done.')\n",
    "        f = h5py.File(snapPath(basePath, snapNum, fileNum), 'r')\n",
    "\n",
    "        # no particles of requested type in this file chunk?\n",
    "        if gName not in f:\n",
    "            f.close()\n",
    "            fileNum += 1\n",
    "            fileOff  = 0\n",
    "            continue\n",
    "\n",
    "        # set local read length for this file chunk, truncate to be within the local size\n",
    "        numTypeLocal = f['Header'].attrs['NumPart_ThisFile'][ptNum]\n",
    "\n",
    "        numToReadLocal = numToRead\n",
    "\n",
    "        if fileOff + numToReadLocal > numTypeLocal:\n",
    "            numToReadLocal = numTypeLocal - fileOff\n",
    "\n",
    "        #print('['+str(fileNum).rjust(3)+'] off='+str(fileOff)+' read ['+str(numToReadLocal)+\\\n",
    "        #      '] of ['+str(numTypeLocal)+'] remaining = '+str(numToRead-numToReadLocal))\n",
    "\n",
    "        # loop over each requested field for this particle type\n",
    "        for i, field in enumerate(fields):\n",
    "            # read data local to the current file\n",
    "            if mdi is None or mdi[i] is None:\n",
    "                result[field][wOffset:wOffset+numToReadLocal] = f[gName][field][fileOff:fileOff+numToReadLocal]\n",
    "            else:\n",
    "                result[field][wOffset:wOffset+numToReadLocal] = f[gName][field][fileOff:fileOff+numToReadLocal, mdi[i]]\n",
    "\n",
    "        wOffset   += numToReadLocal\n",
    "        numToRead -= numToReadLocal\n",
    "        fileNum   += 1\n",
    "        fileOff    = 0  # start at beginning of all file chunks other than the first\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    # verify we read the correct number\n",
    "    if origNumToRead != wOffset:\n",
    "        raise Exception(\"Read [\"+str(wOffset)+\"] particles, but was expecting [\"+str(origNumToRead)+\"]\")\n",
    "\n",
    "    # only a single field? then return the array instead of a single item dict\n",
    "    if sq and len(fields) == 1:\n",
    "        return result[fields[0]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def getSnapOffsets(basePath, snapNum, id, type):\n",
    "    \"\"\" Compute offsets within snapshot for a particular group/subgroup. \"\"\"\n",
    "    r = {}\n",
    "\n",
    "    # old or new format\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        # use separate 'offsets_nnn.hdf5' files\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            groupFileOffsets = f['FileOffsets/'+type][()]\n",
    "            r['snapOffsets'] = np.transpose(f['FileOffsets/SnapByType'][()])  # consistency\n",
    "    else:\n",
    "        # load groupcat chunk offsets from header of first file\n",
    "        with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "            groupFileOffsets = f['Header'].attrs['FileOffsets_'+type]\n",
    "            r['snapOffsets'] = f['Header'].attrs['FileOffsets_Snap']\n",
    "\n",
    "    # calculate target groups file chunk which contains this id\n",
    "    groupFileOffsets = int(id) - groupFileOffsets\n",
    "    fileNum = np.max(np.where(groupFileOffsets >= 0))\n",
    "    groupOffset = groupFileOffsets[fileNum]\n",
    "\n",
    "    # load the length (by type) of this group/subgroup from the group catalog\n",
    "    with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "        r['lenType'] = f[type][type+'LenType'][groupOffset, :]\n",
    "\n",
    "    # old or new format: load the offset (by type) of this group/subgroup within the snapshot\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            r['offsetType'] = f[type+'/SnapByType'][id, :]\n",
    "\n",
    "            # add TNG-Cluster specific offsets if present\n",
    "            if 'OriginalZooms' in f:\n",
    "                for key in f['OriginalZooms']:\n",
    "                    r[key] = f['OriginalZooms'][key][()] \n",
    "    else:\n",
    "        with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "            r['offsetType'] = f['Offsets'][type+'_SnapByType'][groupOffset, :]\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def loadSubhalo(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type for a specific subhalo\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load subhalo length, compute offset, call loadSubset\n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Subhalo\")\n",
    "    return loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "\n",
    "def loadHalo(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type for a specific halo\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load halo length, compute offset, call loadSubset\n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Group\")\n",
    "    return loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "\n",
    "def loadOriginalZoom(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type corresponding to an\n",
    "        original (entire) zoom simulation. TNG-Cluster specific.\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load fuzz length, compute offset, call loadSubset                                                                     \n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Group\")\n",
    "\n",
    "    # identify original halo ID and corresponding index\n",
    "    halo = loadSingle(basePath, snapNum, haloID=id)\n",
    "    assert 'GroupOrigHaloID' in halo, 'Error: loadOriginalZoom() only for the TNG-Cluster simulation.'\n",
    "    orig_index = np.where(subset['HaloIDs'] == halo['GroupOrigHaloID'])[0][0]\n",
    "\n",
    "    # (1) load all FoF particles/cells\n",
    "    subset['lenType'] = subset['GroupsTotalLengthByType'][orig_index, :]\n",
    "    subset['offsetType'] = subset['GroupsSnapOffsetByType'][orig_index, :]\n",
    "\n",
    "    data1 = loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "    # (2) load all non-FoF particles/cells\n",
    "    subset['lenType'] = subset['OuterFuzzTotalLengthByType'][orig_index, :]\n",
    "    subset['offsetType'] = subset['OuterFuzzSnapOffsetByType'][orig_index, :]\n",
    "\n",
    "    data2 = loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "    # combine and return\n",
    "    if isinstance(data1, np.ndarray):\n",
    "        return np.concatenate((data1,data2), axis=0)\n",
    "    \n",
    "    data = {'count':data1['count']+data2['count']}\n",
    "    for key in data1.keys():\n",
    "        if key == 'count': continue\n",
    "        data[key] = np.concatenate((data1[key],data2[key]), axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_particles_num(basePath, snapNum, subhaloID):\n",
    "    basePath = os.path.join(basePath, \"TNG100-1\", \"output\", )\n",
    "    partType = 'gas'\n",
    "    subset = getSnapOffsets(basePath, snapNum, subhaloID, \"Subhalo\")\n",
    "    subhalo = loadSubset(basePath, snapNum, partType, subset=subset)\n",
    "    gas = il.snapshot.loadSubhalo(basePath, snapNum, subhaloID, partType)\n",
    "    print(subhaloID)\n",
    "    if 'Coordinates' in gas.keys():\n",
    "        gas_num = len(gas['Coordinates'])\n",
    "    else:\n",
    "        gas_num = 0\n",
    "    return gas_num\n",
    "\n",
    "part_numbers = [get_particles_num(basepath, snap, int(subhalo_id)) for subhalo_id in subhalo_ids]\n",
    "print(len(part_numbers))\n",
    "\n",
    "#get_particles_num(basePath, snapNum, subhaloID)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(9 in [0, 1, 2, 3, 4, 5, 6, 7, 8, 22, 30, 56, 15, 23, 31, 57, 16, 24, 32, 58, 17, 111, 180, 142, 85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casa6.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
