{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database contains 12535 subhalos\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "117\n",
      "118\n",
      "120\n",
      "121\n",
      "123\n",
      "124\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "137\n",
      "138\n",
      "141\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "149\n",
      "150\n",
      "152\n",
      "153\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "164\n",
      "165\n",
      "167\n",
      "173\n",
      "177\n",
      "178\n",
      "179\n",
      "181\n",
      "186\n",
      "188\n",
      "189\n",
      "192\n",
      "194\n",
      "196\n",
      "200\n",
      "202\n",
      "206\n",
      "208\n",
      "215\n",
      "216\n",
      "221\n",
      "224\n",
      "231\n",
      "238\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "246\n",
      "247\n",
      "254\n",
      "255\n",
      "260\n",
      "265\n",
      "267\n",
      "274\n",
      "281\n",
      "288\n",
      "292\n",
      "298\n",
      "304\n",
      "308\n",
      "317\n",
      "322\n",
      "327\n",
      "17185\n",
      "17186\n",
      "17187\n",
      "17188\n",
      "17189\n",
      "17190\n",
      "17191\n",
      "17192\n",
      "17193\n",
      "17194\n",
      "17195\n",
      "17196\n",
      "17197\n",
      "17198\n",
      "17199\n",
      "17200\n",
      "17201\n",
      "17202\n",
      "17203\n",
      "17204\n",
      "17205\n",
      "17206\n",
      "17207\n",
      "17208\n",
      "17209\n",
      "17210\n",
      "17211\n",
      "17212\n",
      "17213\n",
      "17214\n",
      "17215\n",
      "17216\n",
      "17217\n",
      "17218\n",
      "17219\n",
      "17220\n",
      "17221\n",
      "17222\n",
      "17223\n",
      "17224\n",
      "17225\n",
      "17226\n",
      "17227\n",
      "17228\n",
      "17229\n",
      "17230\n",
      "17231\n",
      "17232\n",
      "17233\n",
      "17234\n",
      "17235\n",
      "17236\n",
      "17237\n",
      "17238\n",
      "17239\n",
      "17240\n",
      "17241\n",
      "17242\n",
      "17243\n",
      "17244\n",
      "17245\n",
      "17246\n",
      "17247\n",
      "17248\n",
      "17249\n",
      "17250\n",
      "17251\n",
      "17252\n",
      "17253\n",
      "17254\n",
      "17255\n",
      "17256\n",
      "17257\n",
      "17258\n",
      "17260\n",
      "17261\n",
      "17262\n",
      "17263\n",
      "17265\n",
      "17266\n",
      "17267\n",
      "17268\n",
      "17269\n",
      "17270\n",
      "17271\n",
      "17272\n",
      "17273\n",
      "17274\n",
      "17275\n",
      "17276\n",
      "17277\n",
      "17281\n",
      "17283\n",
      "17284\n",
      "17286\n",
      "17291\n",
      "17293\n",
      "17294\n",
      "17296\n",
      "17297\n",
      "17301\n",
      "17303\n",
      "17304\n",
      "17307\n",
      "17310\n",
      "17311\n",
      "17316\n",
      "17317\n",
      "17318\n",
      "17321\n",
      "17324\n",
      "17326\n",
      "17327\n",
      "17328\n",
      "17331\n",
      "17333\n",
      "17336\n",
      "17339\n",
      "17346\n",
      "17349\n",
      "17350\n",
      "17351\n",
      "17352\n",
      "17354\n",
      "17355\n",
      "17358\n",
      "17362\n",
      "17376\n",
      "17378\n",
      "17382\n",
      "17383\n",
      "17397\n",
      "17407\n",
      "17408\n",
      "17422\n",
      "17427\n",
      "17431\n",
      "17436\n",
      "Downloading Snapshot 9 ...\n",
      "Done.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '/home/astro/Documents/TNGData/TNG100-1/output/snapdir_099/snapshot_099.9.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/astro/Documents/GitHub/ALMASim/tester.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=462'>463</a>\u001b[0m         gas_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=463'>464</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m gas_num\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=465'>466</a>\u001b[0m part_numbers \u001b[39m=\u001b[39m [get_particles_num(basepath, snap, \u001b[39mint\u001b[39m(subhalo_id)) \u001b[39mfor\u001b[39;00m subhalo_id \u001b[39min\u001b[39;00m subhalo_ids]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=466'>467</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(part_numbers))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=468'>469</a>\u001b[0m \u001b[39m#get_particles_num(basePath, snapNum, subhaloID)\u001b[39;00m\n",
      "\u001b[1;32m/home/astro/Documents/GitHub/ALMASim/tester.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=462'>463</a>\u001b[0m         gas_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=463'>464</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m gas_num\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=465'>466</a>\u001b[0m part_numbers \u001b[39m=\u001b[39m [get_particles_num(basepath, snap, \u001b[39mint\u001b[39;49m(subhalo_id)) \u001b[39mfor\u001b[39;00m subhalo_id \u001b[39min\u001b[39;00m subhalo_ids]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=466'>467</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(part_numbers))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=468'>469</a>\u001b[0m \u001b[39m#get_particles_num(basePath, snapNum, subhaloID)\u001b[39;00m\n",
      "\u001b[1;32m/home/astro/Documents/GitHub/ALMASim/tester.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=454'>455</a>\u001b[0m partType \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgas\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=455'>456</a>\u001b[0m subset \u001b[39m=\u001b[39m getSnapOffsets(basePath, snapNum, subhaloID, \u001b[39m\"\u001b[39m\u001b[39mSubhalo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=456'>457</a>\u001b[0m subhalo \u001b[39m=\u001b[39m loadSubset(basePath, snapNum, partType, subset\u001b[39m=\u001b[39;49msubset)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=457'>458</a>\u001b[0m gas \u001b[39m=\u001b[39m il\u001b[39m.\u001b[39msnapshot\u001b[39m.\u001b[39mloadSubhalo(basePath, snapNum, subhaloID, partType)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=458'>459</a>\u001b[0m \u001b[39mprint\u001b[39m(subhaloID)\n",
      "\u001b[1;32m/home/astro/Documents/GitHub/ALMASim/tester.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=312'>313</a>\u001b[0m     subprocess\u001b[39m.\u001b[39mcheck_call(cmd, shell\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=313'>314</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=314'>315</a>\u001b[0m f \u001b[39m=\u001b[39m h5py\u001b[39m.\u001b[39;49mFile(snapPath(basePath, snapNum, fileNum), \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=316'>317</a>\u001b[0m \u001b[39m# no particles of requested type in this file chunk?\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bastro-workstation/home/astro/Documents/GitHub/ALMASim/tester.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=317'>318</a>\u001b[0m \u001b[39mif\u001b[39;00m gName \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m f:\n",
      "File \u001b[0;32m~/almasim/lib/python3.8/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/almasim/lib/python3.8/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/home/astro/Documents/TNGData/TNG100-1/output/snapdir_099/snapshot_099.9.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import simulator as sm\n",
    "import illustris_python as il\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import subprocess\n",
    "file = 'morphologies_deeplearn.hdf5'\n",
    "db = sm.get_data_from_hdf(file)\n",
    "subhalo_ids = db['SubhaloID'].values\n",
    "print('Database contains %d subhalos' % len(subhalo_ids))\n",
    "basePath = '/home/astro/Documents/TNGData'\n",
    "snapNum = 99\n",
    "\n",
    "import six\n",
    "from os.path import isfile,expanduser\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def partTypeNum(partType):\n",
    "    \"\"\" Mapping between common names and numeric particle types. \"\"\"\n",
    "    if str(partType).isdigit():\n",
    "        return int(partType)\n",
    "        \n",
    "    if str(partType).lower() in ['gas','cells']:\n",
    "        return 0\n",
    "    if str(partType).lower() in ['dm','darkmatter']:\n",
    "        return 1\n",
    "    if str(partType).lower() in ['dmlowres']:\n",
    "        return 2 # only zoom simulations, not present in full periodic boxes\n",
    "    if str(partType).lower() in ['tracer','tracers','tracermc','trmc']:\n",
    "        return 3\n",
    "    if str(partType).lower() in ['star','stars','stellar']:\n",
    "        return 4 # only those with GFM_StellarFormationTime>0\n",
    "    if str(partType).lower() in ['wind']:\n",
    "        return 4 # only those with GFM_StellarFormationTime<0\n",
    "    if str(partType).lower() in ['bh','bhs','blackhole','blackholes']:\n",
    "        return 5\n",
    "    \n",
    "    raise Exception(\"Unknown particle type name.\")\n",
    "\n",
    "\n",
    "def gcPath(basePath, snapNum, chunkNum=0):\n",
    "    \"\"\" Return absolute path to a group catalog HDF5 file (modify as needed). \"\"\"\n",
    "    gcPath = basePath + '/groups_%03d/' % snapNum\n",
    "    filePath1 = gcPath + 'groups_%03d.%d.hdf5' % (snapNum, chunkNum)\n",
    "    filePath2 = gcPath + 'fof_subhalo_tab_%03d.%d.hdf5' % (snapNum, chunkNum)\n",
    "\n",
    "    if isfile(expanduser(filePath1)):\n",
    "        return filePath1\n",
    "    return filePath2\n",
    "\n",
    "\n",
    "def offsetPath(basePath, snapNum):\n",
    "    \"\"\" Return absolute path to a separate offset file (modify as needed). \"\"\"\n",
    "    offsetPath = basePath + '/../postprocessing/offsets/offsets_%03d.hdf5' % snapNum\n",
    "\n",
    "    return offsetPath\n",
    "\n",
    "\n",
    "def loadObjects(basePath, snapNum, gName, nName, fields):\n",
    "    \"\"\" Load either halo or subhalo information from the group catalog. \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # make sure fields is not a single element\n",
    "    if isinstance(fields, six.string_types):\n",
    "        fields = [fields]\n",
    "\n",
    "    # load header from first chunk\n",
    "    with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "\n",
    "        if 'N'+nName+'_Total' not in header and nName == 'subgroups':\n",
    "            nName = 'subhalos' # alternate convention\n",
    "\n",
    "        result['count'] = f['Header'].attrs['N' + nName + '_Total']\n",
    "\n",
    "        if not result['count']:\n",
    "            print('warning: zero groups, empty return (snap=' + str(snapNum) + ').')\n",
    "            return result\n",
    "\n",
    "        # if fields not specified, load everything\n",
    "        if not fields:\n",
    "            fields = list(f[gName].keys())\n",
    "\n",
    "        for field in fields:\n",
    "            # verify existence\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Group catalog does not have requested field [\" + field + \"]!\")\n",
    "\n",
    "            # replace local length with global\n",
    "            shape = list(f[gName][field].shape)\n",
    "            shape[0] = result['count']\n",
    "\n",
    "            # allocate within return dict\n",
    "            result[field] = np.zeros(shape, dtype=f[gName][field].dtype)\n",
    "\n",
    "    # loop over chunks\n",
    "    wOffset = 0\n",
    "\n",
    "    for i in range(header['NumFiles']):\n",
    "        f = h5py.File(gcPath(basePath, snapNum, i), 'r')\n",
    "\n",
    "        if not f['Header'].attrs['N'+nName+'_ThisFile']:\n",
    "            continue  # empty file chunk\n",
    "\n",
    "        # loop over each requested field\n",
    "        for field in fields:\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Group catalog does not have requested field [\" + field + \"]!\")\n",
    "\n",
    "            # shape and type\n",
    "            shape = f[gName][field].shape\n",
    "\n",
    "            # read data local to the current file\n",
    "            if len(shape) == 1:\n",
    "                result[field][wOffset:wOffset+shape[0]] = f[gName][field][0:shape[0]]\n",
    "            else:\n",
    "                result[field][wOffset:wOffset+shape[0], :] = f[gName][field][0:shape[0], :]\n",
    "\n",
    "        wOffset += shape[0]\n",
    "        f.close()\n",
    "\n",
    "    # only a single field? then return the array instead of a single item dict\n",
    "    if len(fields) == 1:\n",
    "        return result[fields[0]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def loadSubhalos(basePath, snapNum, fields=None):\n",
    "    \"\"\" Load all subhalo information from the entire group catalog for one snapshot\n",
    "       (optionally restrict to a subset given by fields). \"\"\"\n",
    "\n",
    "    return loadObjects(basePath, snapNum, \"Subhalo\", \"subgroups\", fields)\n",
    "\n",
    "\n",
    "def loadHalos(basePath, snapNum, fields=None):\n",
    "    \"\"\" Load all halo information from the entire group catalog for one snapshot\n",
    "       (optionally restrict to a subset given by fields). \"\"\"\n",
    "\n",
    "    return loadObjects(basePath, snapNum, \"Group\", \"groups\", fields)\n",
    "\n",
    "\n",
    "def loadHeader(basePath, snapNum):\n",
    "    \"\"\" Load the group catalog header. \"\"\"\n",
    "    with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "\n",
    "    return header\n",
    "\n",
    "\n",
    "def load(basePath, snapNum):\n",
    "    \"\"\" Load complete group catalog all at once. \"\"\"\n",
    "    r = {}\n",
    "    r['subhalos'] = loadSubhalos(basePath, snapNum)\n",
    "    r['halos']    = loadHalos(basePath, snapNum)\n",
    "    r['header']   = loadHeader(basePath, snapNum)\n",
    "    return r\n",
    "\n",
    "\n",
    "def loadSingle(basePath, snapNum, haloID=-1, subhaloID=-1):\n",
    "    \"\"\" Return complete group catalog information for one halo or subhalo. \"\"\"\n",
    "    if (haloID < 0 and subhaloID < 0) or (haloID >= 0 and subhaloID >= 0):\n",
    "        raise Exception(\"Must specify either haloID or subhaloID (and not both).\")\n",
    "\n",
    "    gName = \"Subhalo\" if subhaloID >= 0 else \"Group\"\n",
    "    searchID = subhaloID if subhaloID >= 0 else haloID\n",
    "\n",
    "    # old or new format\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        # use separate 'offsets_nnn.hdf5' files\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            offsets = f['FileOffsets/'+gName][()]\n",
    "    else:\n",
    "        # use header of group catalog\n",
    "        with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "            offsets = f['Header'].attrs['FileOffsets_'+gName]\n",
    "\n",
    "    offsets = searchID - offsets\n",
    "    fileNum = np.max(np.where(offsets >= 0))\n",
    "    groupOffset = offsets[fileNum]\n",
    "\n",
    "    # load halo/subhalo fields into a dict\n",
    "    result = {}\n",
    "\n",
    "    with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "        for haloProp in f[gName].keys():\n",
    "            result[haloProp] = f[gName][haloProp][groupOffset]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def snapPath(basePath, snapNum, chunkNum=0):\n",
    "    \"\"\" Return absolute path to a snapshot HDF5 file (modify as needed). \"\"\"\n",
    "    snapPath = basePath + '/snapdir_' + str(snapNum).zfill(3) + '/'\n",
    "    filePath1 = snapPath + 'snap_' + str(snapNum).zfill(3) + '.' + str(chunkNum) + '.hdf5'\n",
    "    filePath2 = filePath1.replace('/snap_', '/snapshot_')\n",
    "\n",
    "    if isfile(filePath1):\n",
    "        return filePath1\n",
    "    return filePath2\n",
    "\n",
    "\n",
    "def getNumPart(header):\n",
    "    \"\"\" Calculate number of particles of all types given a snapshot header. \"\"\"\n",
    "    if 'NumPart_Total_HighWord' not in header:\n",
    "        return header['NumPart_Total'] # new uint64 convention\n",
    "\n",
    "    nTypes = 6\n",
    "\n",
    "    nPart = np.zeros(nTypes, dtype=np.int64)\n",
    "    for j in range(nTypes):\n",
    "        nPart[j] = header['NumPart_Total'][j] | (header['NumPart_Total_HighWord'][j] << 32)\n",
    "\n",
    "    return nPart\n",
    "\n",
    "\n",
    "def loadSubset(basePath, snapNum, partType, fields=None, subset=None, mdi=None, sq=True, float32=False):\n",
    "    \"\"\" Load a subset of fields for all particles/cells of a given partType.\n",
    "        If offset and length specified, load only that subset of the partType.\n",
    "        If mdi is specified, must be a list of integers of the same length as fields,\n",
    "        giving for each field the multi-dimensional index (on the second dimension) to load.\n",
    "          For example, fields=['Coordinates', 'Masses'] and mdi=[1, None] returns a 1D array\n",
    "          of y-Coordinates only, together with Masses.\n",
    "        If sq is True, return a numpy array instead of a dict if len(fields)==1.\n",
    "        If float32 is True, load any float64 datatype arrays directly as float32 (save memory). \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    ptNum = partTypeNum(partType)\n",
    "    gName = \"PartType\" + str(ptNum)\n",
    "\n",
    "    # make sure fields is not a single element\n",
    "    if isinstance(fields, six.string_types):\n",
    "        fields = [fields]\n",
    "\n",
    "    # load header from first chunk\n",
    "    with h5py.File(snapPath(basePath, snapNum), 'r') as f:\n",
    "\n",
    "        header = dict(f['Header'].attrs.items())\n",
    "        nPart = getNumPart(header)\n",
    "\n",
    "        # decide global read size, starting file chunk, and starting file chunk offset\n",
    "        if subset:\n",
    "            offsetsThisType = subset['offsetType'][ptNum] - subset['snapOffsets'][ptNum, :]\n",
    "\n",
    "            fileNum = np.max(np.where(offsetsThisType >= 0))\n",
    "            fileOff = offsetsThisType[fileNum]\n",
    "            numToRead = subset['lenType'][ptNum]\n",
    "        else:\n",
    "            fileNum = 0\n",
    "            fileOff = 0\n",
    "            numToRead = nPart[ptNum]\n",
    "\n",
    "        result['count'] = numToRead\n",
    "\n",
    "        if not numToRead:\n",
    "            # print('warning: no particles of requested type, empty return.')\n",
    "            return result\n",
    "\n",
    "        # find a chunk with this particle type\n",
    "        i = 1\n",
    "        while gName not in f:\n",
    "            if os.path.isfile(snapPath(basePath, snapNum, i)):\n",
    "                f = h5py.File(snapPath(basePath, snapNum, i), 'r')\n",
    "            else:\n",
    "                api_key = '8f578b92e700fae3266931f4d785f82c'\n",
    "                url = f'http://www.tng-project.org/api/TNG100-1/files/snapshot-{str(snapNum)}'\n",
    "                subdir = os.path.join('output', 'snapdir_0{}'.format(str(i)))\n",
    "                cmd = f'wget -q --progress=bar  --content-disposition --header=\"API-Key:{api_key}\" {url}.{i}.hdf5'\n",
    "                print(f'Downloading {message} {i} ...')\n",
    "                subprocess.check_call(cmd, shell=True)\n",
    "                print('Done.')\n",
    "                f = h5py.File(snapPath(basePath, snapNum, i), 'r')\n",
    "            i += 1\n",
    "\n",
    "        # if fields not specified, load everything\n",
    "        if not fields:\n",
    "            fields = list(f[gName].keys())\n",
    "\n",
    "        for i, field in enumerate(fields):\n",
    "            # verify existence\n",
    "            if field not in f[gName].keys():\n",
    "                raise Exception(\"Particle type [\"+str(ptNum)+\"] does not have field [\"+field+\"]\")\n",
    "\n",
    "            # replace local length with global\n",
    "            shape = list(f[gName][field].shape)\n",
    "            shape[0] = numToRead\n",
    "\n",
    "            # multi-dimensional index slice load\n",
    "            if mdi is not None and mdi[i] is not None:\n",
    "                if len(shape) != 2:\n",
    "                    raise Exception(\"Read error: mdi requested on non-2D field [\"+field+\"]\")\n",
    "                shape = [shape[0]]\n",
    "\n",
    "            # allocate within return dict\n",
    "            dtype = f[gName][field].dtype\n",
    "            if dtype == np.float64 and float32: dtype = np.float32\n",
    "            result[field] = np.zeros(shape, dtype=dtype)\n",
    "\n",
    "    # loop over chunks\n",
    "    wOffset = 0\n",
    "    origNumToRead = numToRead\n",
    "\n",
    "    while numToRead:\n",
    "        if not os.path.isfile(snapPath(basePath, snapNum, fileNum)):\n",
    "            # move directory to the correct directory data !!!\n",
    "            api_key = '8f578b92e700fae3266931f4d785f82c'\n",
    "            url = f'http://www.tng-project.org/api/TNG100-1/files/snapshot-{str(snapNum)}'\n",
    "            subdir = os.path.join('output', 'snapdir_0{}'.format(str(fileNum)))\n",
    "            cmd = f'wget -q --progress=bar  --content-disposition --header=\"API-Key:{api_key}\" {url}.{fileNum}.hdf5'\n",
    "            print(f'Downloading Snapshot {fileNum} ...')\n",
    "            subprocess.check_call(cmd, shell=True)\n",
    "            print('Done.')\n",
    "        f = h5py.File(snapPath(basePath, snapNum, fileNum), 'r')\n",
    "\n",
    "        # no particles of requested type in this file chunk?\n",
    "        if gName not in f:\n",
    "            f.close()\n",
    "            fileNum += 1\n",
    "            fileOff  = 0\n",
    "            continue\n",
    "\n",
    "        # set local read length for this file chunk, truncate to be within the local size\n",
    "        numTypeLocal = f['Header'].attrs['NumPart_ThisFile'][ptNum]\n",
    "\n",
    "        numToReadLocal = numToRead\n",
    "\n",
    "        if fileOff + numToReadLocal > numTypeLocal:\n",
    "            numToReadLocal = numTypeLocal - fileOff\n",
    "\n",
    "        #print('['+str(fileNum).rjust(3)+'] off='+str(fileOff)+' read ['+str(numToReadLocal)+\\\n",
    "        #      '] of ['+str(numTypeLocal)+'] remaining = '+str(numToRead-numToReadLocal))\n",
    "\n",
    "        # loop over each requested field for this particle type\n",
    "        for i, field in enumerate(fields):\n",
    "            # read data local to the current file\n",
    "            if mdi is None or mdi[i] is None:\n",
    "                result[field][wOffset:wOffset+numToReadLocal] = f[gName][field][fileOff:fileOff+numToReadLocal]\n",
    "            else:\n",
    "                result[field][wOffset:wOffset+numToReadLocal] = f[gName][field][fileOff:fileOff+numToReadLocal, mdi[i]]\n",
    "\n",
    "        wOffset   += numToReadLocal\n",
    "        numToRead -= numToReadLocal\n",
    "        fileNum   += 1\n",
    "        fileOff    = 0  # start at beginning of all file chunks other than the first\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    # verify we read the correct number\n",
    "    if origNumToRead != wOffset:\n",
    "        raise Exception(\"Read [\"+str(wOffset)+\"] particles, but was expecting [\"+str(origNumToRead)+\"]\")\n",
    "\n",
    "    # only a single field? then return the array instead of a single item dict\n",
    "    if sq and len(fields) == 1:\n",
    "        return result[fields[0]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def getSnapOffsets(basePath, snapNum, id, type):\n",
    "    \"\"\" Compute offsets within snapshot for a particular group/subgroup. \"\"\"\n",
    "    r = {}\n",
    "\n",
    "    # old or new format\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        # use separate 'offsets_nnn.hdf5' files\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            groupFileOffsets = f['FileOffsets/'+type][()]\n",
    "            r['snapOffsets'] = np.transpose(f['FileOffsets/SnapByType'][()])  # consistency\n",
    "    else:\n",
    "        # load groupcat chunk offsets from header of first file\n",
    "        with h5py.File(gcPath(basePath, snapNum), 'r') as f:\n",
    "            groupFileOffsets = f['Header'].attrs['FileOffsets_'+type]\n",
    "            r['snapOffsets'] = f['Header'].attrs['FileOffsets_Snap']\n",
    "\n",
    "    # calculate target groups file chunk which contains this id\n",
    "    groupFileOffsets = int(id) - groupFileOffsets\n",
    "    fileNum = np.max(np.where(groupFileOffsets >= 0))\n",
    "    groupOffset = groupFileOffsets[fileNum]\n",
    "\n",
    "    # load the length (by type) of this group/subgroup from the group catalog\n",
    "    with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "        r['lenType'] = f[type][type+'LenType'][groupOffset, :]\n",
    "\n",
    "    # old or new format: load the offset (by type) of this group/subgroup within the snapshot\n",
    "    if 'fof_subhalo' in gcPath(basePath, snapNum):\n",
    "        with h5py.File(offsetPath(basePath, snapNum), 'r') as f:\n",
    "            r['offsetType'] = f[type+'/SnapByType'][id, :]\n",
    "\n",
    "            # add TNG-Cluster specific offsets if present\n",
    "            if 'OriginalZooms' in f:\n",
    "                for key in f['OriginalZooms']:\n",
    "                    r[key] = f['OriginalZooms'][key][()] \n",
    "    else:\n",
    "        with h5py.File(gcPath(basePath, snapNum, fileNum), 'r') as f:\n",
    "            r['offsetType'] = f['Offsets'][type+'_SnapByType'][groupOffset, :]\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def loadSubhalo(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type for a specific subhalo\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load subhalo length, compute offset, call loadSubset\n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Subhalo\")\n",
    "    return loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "\n",
    "def loadHalo(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type for a specific halo\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load halo length, compute offset, call loadSubset\n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Group\")\n",
    "    return loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "\n",
    "def loadOriginalZoom(basePath, snapNum, id, partType, fields=None):\n",
    "    \"\"\" Load all particles/cells of one type corresponding to an\n",
    "        original (entire) zoom simulation. TNG-Cluster specific.\n",
    "        (optionally restricted to a subset fields). \"\"\"\n",
    "    # load fuzz length, compute offset, call loadSubset                                                                     \n",
    "    subset = getSnapOffsets(basePath, snapNum, id, \"Group\")\n",
    "\n",
    "    # identify original halo ID and corresponding index\n",
    "    halo = loadSingle(basePath, snapNum, haloID=id)\n",
    "    assert 'GroupOrigHaloID' in halo, 'Error: loadOriginalZoom() only for the TNG-Cluster simulation.'\n",
    "    orig_index = np.where(subset['HaloIDs'] == halo['GroupOrigHaloID'])[0][0]\n",
    "\n",
    "    # (1) load all FoF particles/cells\n",
    "    subset['lenType'] = subset['GroupsTotalLengthByType'][orig_index, :]\n",
    "    subset['offsetType'] = subset['GroupsSnapOffsetByType'][orig_index, :]\n",
    "\n",
    "    data1 = loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "    # (2) load all non-FoF particles/cells\n",
    "    subset['lenType'] = subset['OuterFuzzTotalLengthByType'][orig_index, :]\n",
    "    subset['offsetType'] = subset['OuterFuzzSnapOffsetByType'][orig_index, :]\n",
    "\n",
    "    data2 = loadSubset(basePath, snapNum, partType, fields, subset=subset)\n",
    "\n",
    "    # combine and return\n",
    "    if isinstance(data1, np.ndarray):\n",
    "        return np.concatenate((data1,data2), axis=0)\n",
    "    \n",
    "    data = {'count':data1['count']+data2['count']}\n",
    "    for key in data1.keys():\n",
    "        if key == 'count': continue\n",
    "        data[key] = np.concatenate((data1[key],data2[key]), axis=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_particles_num(basePath, snapNum, subhaloID):\n",
    "    basePath = os.path.join(basePath, \"TNG100-1\", \"output\", )\n",
    "    partType = 'gas'\n",
    "    subset = getSnapOffsets(basePath, snapNum, subhaloID, \"Subhalo\")\n",
    "    subhalo = loadSubset(basePath, snapNum, partType, subset=subset)\n",
    "    gas = il.snapshot.loadSubhalo(basePath, snapNum, subhaloID, partType)\n",
    "    print(subhaloID)\n",
    "    if 'Coordinates' in gas.keys():\n",
    "        gas_num = len(gas['Coordinates'])\n",
    "    else:\n",
    "        gas_num = 0\n",
    "    return gas_num\n",
    "\n",
    "part_numbers = [get_particles_num(basepath, snap, int(subhalo_id)) for subhalo_id in subhalo_ids]\n",
    "print(len(part_numbers))\n",
    "\n",
    "#get_particles_num(basePath, snapNum, subhaloID)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(9 in [0, 1, 2, 3, 4, 5, 6, 7, 8, 22, 30, 56, 15, 23, 31, 57, 16, 24, 32, 58, 17, 111, 180, 142, 85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casa6.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
